# <font color = "purple">Preprocessing: Data Scaling</font>


## **Background and Context**:
Scaling is the process by which we <u>normalize the numeric range of the atttributes of our data.</u> 
   - _example_, we might adjust the scale of our data such that the highest value is 1 and the smallest value is 0 (this is referred to as min-max scaling).
   
### <font color ="darkgreen">**Remember we pass only numeric values to the model, not the units. So model measures distance based on magnitude of numeric data regardless of units**</font>

# <font color ="darkyellow">Why Scale Data?</font>
* In general, working with the data in it's original units is preferred. However, there are a number of occasions where we might wish to work with scaled data instead:

* We might wish to visualize the combination of 2 variables with different scales.
* Some statistical tests assume normality of the data. We could apply a non-linear scaler to a non-normally distributed feature so that this assumption is met.
* If two features have very different scales the feature with the larger units will have a larger impact on some model types (anywhere distance is measured).
* Sometimes scaling the data can provide a better interpretation than the original units. Times when a log scale is helpful are an example of this.
* If we wish to combine features with different units it could be helpful to first scale both features to the same units.



## <font color="darkcyan">How do we scale the data?</font>
* Data is scaled with either a linear or non-linear scaling method. A scaling method is typically a mathematical formula that can be used to transform a data point from the original dataset into a datapoint in the scaled dataset. A common convention is to notate the transformed data as xâ€² and the original as x.

* Note that features are scaled independently. That is, the way we scale one feature does not impact the way we scale another feature. Put another way, imagine we have two features we wish to scale: x1 and x2. The mean of x1 plays no role in the way we scale x2.

* Most scaling methods involve the calculation of a parameter from the dataset, for example the maximum data point, or the mean of the data. When calculating these parameters, it is important that they are derived from the training dataset. Once we know these parameters, we can apply the same scaling to every data point in the validate and test splits.

* Most of the scaling methods we'll discuss have implementations in the sklearn.preprocessing module. They objects there are used like many other sklearn objects: first by fitting and then by transforming. A fit object can be used to either scale an unscaled feature or to transform a scaled feature back to the original units.


## <font color="orange">Linear vs Non-linear Scalers</font>
* The types of scaling we can perform on our data fall into two categories: linear and non-linear. In a linear scaling operation there is a linear correspondance between the original and the scaled value. A linear scaling operation maintains the "shape" of the distribution data and the space between data points is preserved. A non-linear scaling operation changes the shape of the data, and the distance between the points is not preserved.

* Usually we will use a linear scaler, but non-linear scalers can be useful when changing the shape of the data is desireable, for example, when we wish to use a statistical test that assumes normality, but our data is not normally distributed.



# <font color="darkslateblue">Setup</font>

``` python

import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
import sklearn.preprocessing
from sklearn.model_selection import train_test_split
import pandas as pd

np.random.seed(123)

x = stats.skewnorm(7).rvs(1500) * 10 + 100
x = x.reshape(-1, 1)

plt.hist(x, bins=25,ec='black')
print('Here is a histogram of the dataset we will be working with.')


![split-and-scale_histogram.png](attachment:split-and-scale_histogram.png)